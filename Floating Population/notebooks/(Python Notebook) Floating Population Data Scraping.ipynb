{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reducing Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "consumerKey = 'hnuzMJ1QGE3TcjlYiXohfA6vkGb55OT9xKtTCcd31L4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Summary Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to obtain the floating population data (point type) from [Tokyo Public Transportation Open Data Challenge](https://ckan-tokyochallenge.odpt.org/dataset/o_fpd_point-agoop). This data however is very large, and we undesirable to store in memory. Thus, we need to reduce the data along the way and to produce the dataset we want. First lets download the summary dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_url = 'https://api-tokyochallenge.odpt.org/api/v4/files/Agoop/data/Summary.csv?acl:consumerKey={}'\n",
    "df_summary = pd.read_csv(summary_url.format(consumerKey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.date = [datetime.strptime(_d, '%Y/%m/%d') for _d in df_summary.date]\n",
    "plt.figure(figsize=(16,8), dpi=80)\n",
    "plt.subplot(121)\n",
    "sns.lineplot(x='date', y='dailyid_count', \n",
    "             data=df_summary,\n",
    "             estimator=None\n",
    "            )\n",
    "plt.title('Daily Id count')\n",
    "plt.subplot(122)\n",
    "sns.lineplot(x='date', y='log_count', \n",
    "             data=df_summary,\n",
    "             estimator=None\n",
    "            )\n",
    "plt.title('Daily log count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of the data is from 2018/10/01 to 2019/09/30 (365 days). Our variable of interset is the dailyid_count. Our objective is to gather 37 days worth of data.\n",
    "\n",
    "From the plot we can see that the data from June to August seems relatively stable. They don't decrease or increase suddenly. Thus we will gather data from 2019/06/01 to 2019/7/7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citymaster_url = \"https://api-tokyochallenge.odpt.org/api/v4/files/Agoop/data/prefcode_citycode_master_UTF-8.csv?acl:consumerKey={}\"\n",
    "df_citycode = pd.read_csv(citymaster_url.format(consumerKey))\n",
    "\n",
    "tokyo_32_dict = dict(df_citycode[df_citycode.prefname=='東京都'].loc[:,['cityname', 'citycode']].values[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "url = 'https://api-tokyochallenge.odpt.org/api/v4/files/Agoop/data/PDP_20190721_10.csv?acl:consumerKey='\n",
    "df_test = pd.read_csv(url + consumerKey)\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to subset the dataframe to the cities we are interested in so as to reduce the processing load in the aggregation process. The city codes we are interested in is contained within values of the `tokyo_32_dict`.\n",
    "We can do this smartly by setting the `citycode` as the index and use the `.loc` method to subset the data. To de-index `citycode` we simply call the `reset_index()` method.\n",
    "\n",
    "Next, we want to find the the count of unique `dailyid` for each `citycode`. We can do this grouping by the `citycode` and then by using `nunique` method on the `dailyid` column. Also, since we only need the id counts, we will remove all other columns by subsetting only the `dailyid` column.\n",
    "\n",
    "finally we can encapsulate this whole process into a function and pass it to the preprocessing chain using the `pipe` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_id_by_city(df):\n",
    "    \"\"\"\n",
    "    Counts the unique daily id within each city\n",
    "    \"\"\"\n",
    "    return (df.set_index(['citycode'])\n",
    "            .loc[tokyo_32_dict.values(),['dailyid']]\n",
    "            .reset_index()\n",
    "            .groupby(['citycode'])\n",
    "            .nunique(['dailyid'])\n",
    "            .drop(['citycode'], axis=1)\n",
    "            .reset_index()\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to add the date and time in our dataset in order to plot the time series. However this information is contained when we download the dataset from the database. Ideally, we want to automatically generate the datetime strings for a certain range. \n",
    "\n",
    "We want to take strings to set our beginning and set how many days worth or data we want to obtain. We can use `data_range` method in python to obtain a range of dates(we can also set the frequency) and then use the `strftime` function in `datetime` library to convert them into the string we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_timestamp(df, date):\n",
    "    df['datetime']  = date\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the pandas `pipe` the create a process chain. This makes the code much more readble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_test\n",
    " .pipe(count_id_by_city)\n",
    " .pipe(append_timestamp, '2019-07-21')\n",
    " .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(start: str, days: int):\n",
    "    \"\"\"\n",
    "    start: starting date in 'yyyy-mm-dd' format\n",
    "    days: Number of days of data we want to obtain \n",
    "    \"\"\"\n",
    "    url = 'https://api-tokyochallenge.odpt.org/api/v4/files/Agoop/data/PDP_{}.csv?acl:consumerKey={}'\n",
    "    consumerKey = 'hnuzMJ1QGE3TcjlYiXohfA6vkGb55OT9xKtTCcd31L4'\n",
    "    \n",
    "    periods = days * 24\n",
    "    df_list = []\n",
    "    for date in pd.date_range(start=start, periods=periods, freq='H').tolist():\n",
    "        date_str = datetime.strftime(date, \"%Y%m%d_%H\")\n",
    "        try:\n",
    "            df = (pd.read_csv(url.format(date_str, consumerKey))\n",
    "                  .pipe(count_id_by_city)\n",
    "                  .pipe(append_timestamp, date)\n",
    "                 )\n",
    "            df_list.append(df)\n",
    "            \n",
    "            print('getting data from: {}'.format(date))\n",
    "            \n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data('2019-06-01', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
